# -*- coding: utf-8 -*-
"""HullDataScience_Assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZOYon6b_fzBz6Ebdp-2gKKgEmdrsajUP

Predicting Vehicle Trim and Dealer Listing Price
"""

# Import all of the libraries that will be used for this analysis

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.model_selection
sklearn.model_selection.KFold
from sklearn.model_selection import KFold
import statsmodels.api as sm
import xgboost as xgb
from xgboost.sklearn import (XGBClassifier,
                             XGBRegressor)
from xgboost import (XGBClassifier,
                     XGBRegressor)

from sklearn.metrics import (r2_score,
                             mean_squared_error,
                             mean_absolute_error,
                             median_absolute_error,
                             roc_auc_score,
                             auc,
                             classification_report,
                             confusion_matrix,
                             roc_curve)
from sklearn.metrics import (accuracy_score,
                             roc_auc_score,
                             r2_score,
                             mean_squared_error)
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import (mutual_info_classif,
                                       mutual_info_regression)
from sklearn.preprocessing import (OneHotEncoder,
                                   StandardScaler,
                                   LabelEncoder,
                                   label_binarize)
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.model_selection import (KFold,
                                     train_test_split)
from sklearn.model_selection import train_test_split
from statistics import mean
import hyperopt
from hyperopt import (fmin,
                      tpe,
                      hp,
                      STATUS_OK,
                      Trials)
import warnings

warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

# Load the data

training_data = pd.read_csv('/content/Training_Dataset.csv')
training_data.info()

print(training_data.columns)

# Create command for variables that will be binned
# The bin commands are used on both the training and test set
# This is to process the variables to make sure they are value added

def bin_sellerlistsrc(series):
  series.loc[series.isna()] = 'other'

  series = series.str.lower()
  series.loc[series.str.contains('digital')] = 'dmi'
  series.loc[series.str.contains('command')] = 'icc'
  series.loc[series.str.contains('home')] = 'homenet'
  series.loc[series.str.contains('certified')] = 'certified'
  series.loc[(series != 'dmi') & (series != 'icc') & (series != 'homenet') & (series != 'certified')] = 'other'
  return series

def bin_vehcolorext(series):
  series.loc[series.isna()] = 'other'

  series = series.str.lower()
  series.loc[series.str.contains('pearl')] = 'pearl'
  series.loc[series.str.contains('metallic')] = 'metallic'
  series.loc[series.str.contains('clear')] = 'clear'
  series.loc[(series != 'pearl') & (series != 'metallic') & (series != 'clear')] = 'other'
  return series

def bin_vehcolorint(series):
  keep_colors = ['black', 'unknown', 'beige', 'frost', 'cirrus', 'brown', 'tan', 'maple', 'red', 'plum']
  series.loc[series.isna()] = 'unknown'

  series = series.str.lower()
  for color in keep_colors:
    series.loc[series.str.contains(color)] = color

  series.loc[series.isin(keep_colors) == False] = 'other'
  return series

def bin_vehdrivetrain(series):
    series.loc[series.isna()] = 'unknown'
    series = series.str.lower()
    series.loc[series.str.contains('a')] = 'awd'
    series.loc[series.str.contains('front')] = 'fwd'
    series.loc[series.str.contains('4')] = '4wd'
    series.loc[series.str.contains('four')] = '4wd'
    series.loc[(series != 'awd') & (series != 'fwd') & (series != '4wd')] = 'unknown'
    return series

def bin_vehengine(series):
    series.loc[series.isna()] = 'other'
    series = series.str.lower()
    keep_engines = ['3.6', '5.7', '6.2', '6.4', '3.0', '8']

    for eng in keep_engines:
        series.loc[series.str.contains(eng)] = eng
    series.loc[series.str.contains('hemi')] = '8'
    series.loc[series.isin(keep_engines) == False] = 'other'
    return series

def process_vehfeats(series):
    keep_cols = ['brake', 'airbag', 'input', 'security', 'audio', 'camera', 'bluetooth', 'alloy', 'umin', 'fm', 'cond', 'seat']

    series.loc[series.isna()] = "'none'"
    for j in range(len(series)):
      series.iloc[j] = sorted(series.iloc[j].split("'")[1::2])

    vehfeats_df = pd.DataFrame(series.to_list(), index=series.index)
    onehot_vehfeats = pd.DataFrame(index=series.index, columns=keep_cols)

    for col in onehot_vehfeats.columns:
      for idx in onehot_vehfeats.index:
        tempsum = vehfeats_df.loc[idx, :].str.lower().str.contains(col).sum()
        if col == 'brake':
          tempsum += vehfeats_df.loc[idx,:].str.lower().str.contains('abs').sum()
        if col == 'security':
          tempsum += vehfeats_df.loc[idx,:].str.lower().str.contains('alarm').sum()
        onehot_vehfeats.loc[idx, col] = tempsum > 0

    onehot_vehfeats = onehot_vehfeats.astype(int)
    return onehot_vehfeats


def process_vehhistory(series):
    series.loc[series.isna()] = 'none'

    for j in series.index:
      series.loc[j] = series.loc[j].split(',')

    series_df = pd.DataFrame(series.to_list(), index=series.index)
    for col in series_df.columns:
      series_df.loc[series_df[col].isna(), col] = 'none'

    output_df = pd.DataFrame(index=series.index)
    output_df['owners'] = pd.Series([j[0] for j in series_df.loc[:, 0]], index=series_df.index)
    output_df.loc[output_df['owners']=='n', 'owners'] = np.nan

    output_df['accidents'] = 0
    output_df.loc[series_df.loc[:, 1].str.lower().str.contains('acc'), 'accidents'] = 1

    output_df['use'] = 'none'
    output_df.loc[series_df.loc[:,1].str.lower().str.contains('use'), 'use'] = series_df.loc[series_df.loc[:,1].str.lower().str.contains('use'), 1]
    output_df.loc[series_df.loc[:,2].str.lower().str.contains('use'), 'use'] = series_df.loc[series_df.loc[:,2].str.lower().str.contains('use'), 2]
    output_df.loc[output_df['use'] != 'none'] = 1
    output_df.loc[output_df['use'] == 'none'] = 0

    output_df['title'] = 'none'
    output_df.loc[series_df.loc[:,1].str.lower().str.contains('tit'), 'title'] = series_df.loc[series_df.loc[:,1].str.lower().str.contains('title'), 1]
    output_df.loc[series_df.loc[:,2].str.lower().str.contains('tit'), 'title'] = series_df.loc[series_df.loc[:,2].str.lower().str.contains('title'), 2]
    output_df.loc[series_df.loc[:,3].str.lower().str.contains('tit'), 'title'] = series_df.loc[series_df.loc[:,3].str.lower().str.contains('title'), 3]
    output_df.loc[output_df['title'] != '0'] = 1

    output_df['buyback'] = 0
    for col in series_df.columns:
      output_df.loc[series_df.loc[:,col].str.lower().str.contains('buy'), 'buyback'] = series_df.loc[series_df.loc[:,col].str.lower().str.contains('buy'), col]
    output_df.loc[output_df['buyback'] != 0] = 1

    return output_df['owners'], output_df.drop('owners', axis=1)

def process_vehsellernotes(series):
    series.loc[series.isna()] = 'none'
    tfidf = TfidfVectorizer(max_features=100)
    tfidf_data = tfidf.fit_transform(series)

    return tfidf_data, tfidf

def process_catfeats(catfeats):
    for feat in catfeats.columns:
      if catfeats.loc[:, feat].dtype != 'O':
        catfeats.loc[:, feat] = catfeats.loc[:, feat].astype('O')

    encoder = OneHotEncoder(handle_unknown= 'ignore')
    cats_encoded = encoder.fit_transform(catfeats)
    cats_encoded = pd.DataFrame(cats_encoded.todense(), index=catfeats.index, columns=[item for sublist in encoder.categories_ for item in sublist])
    return encoder, cats_encoded

def process_numfeats(numfeats):
    scaler = StandardScaler()
    nums_scaled = scaler.fit_transform(numfeats)
    nums_scaled = pd.DataFrame(nums_scaled, columns=numfeats.columns, index=numfeats.index)
    return scaler, nums_scaled

def process_tfidf(tfidf_data, tfidf, idx):
    tfidf_df = pd.DataFrame(tfidf_data.toarray(), index=idx, columns=tfidf.vocabulary_)
    return tfidf_df

  # This takes in the raw data of the variables, or features, and process them into features that will work for the modelling
  # This
def preprocess_features(features, dataset='train'):
    log_features = ['sellerrevcnt', 'vehlistdays']
    numeric_features = ['sellerrating', 'sellerrevcnt_log', 'vehlistdays', 'vehlistdays_log', 'vehmileage']
    categorical_features = ['sellerlistsrc', 'sellerrating_round', 'sellerstate', 'vehcertified', 'vehcolorext', 'vehcolorint', 'vehdrivetrain']
    drop_features = ['sellercity', 'sellerispriv', 'sellername', 'sellerzip', 'vehbodystyle', 'vehmodel', 'vehtype', 'vehtransmission']

  # Drop what will not be used

    features = features.drop(drop_features, axis=1)

  # Process columns

    features['sellerlistsrc'] = bin_sellerlistsrc(features['sellerlistsrc'])
    features['vehcolorext'] = bin_vehcolorext(features['vehcolorext'])
    features['vehcolorint'] = bin_vehcolorint(features['vehcolorint'])
    features['vehdrivetrain'] = bin_vehdrivetrain(features['vehdrivetrain'])
    features['vehengine'] = bin_vehengine(features['vehengine'])

    vehfeats_df = process_vehfeats(features['vehfeats']) #merge this dataframe later
    features = features.drop('vehfeats', axis=1)

    owners, vehhistory_df = process_vehhistory(features['vehhistory'])
    features = features.drop('vehhistory', axis=1)

    tfidf_data, tfidf = process_vehsellernotes(features['vehsellernotes'])

    numfeats = features.loc[:, [j for j in features.columns if j in numeric_features]]
    for feat in log_features:

      numfeats[feat + '_log'] = np.log(features.loc[:, feat].add(features.loc[:, feat].pow(2).add(1).pow(0.5)))

    catfeats = features.loc[:, [j for j in features.columns if j in categorical_features]]
    catfeats['sellerrating_round'] = features['sellerrating'].copy().round()
    catfeats['owners'] = owners

    if dataset == 'test':
      return numfeats, catfeats, tfidf_data

    scaler, nums_scaled = process_numfeats(numfeats)
    encoder, cats_coded = process_catfeats(catfeats)
    tfidf_df = process_tfidf(tfidf_data, tfidf, numfeats.index)

    processed_features = pd.concat([nums_scaled, cats_coded, vehfeats_df, tfidf_df], axis=1)

    return scaler, encoder, tfidf, processed_features

# This is to process the test set and make sure all of the variables are usable

def process_testfeatures(scaler, encoder, tfidf, features):
    log_features = ['sellerrevcnt', 'vehlistdays']
    numeric_features = ['sellerrating', 'sellerrevcnt_log', 'vehlistdays', 'vehlistdays_log', 'vehmileage']
    categorical_features = ['sellerlistsrc', 'sellerrating_round', 'sellerstate', 'vehcertified', 'vehcolorext', 'vehcolorint', 'vehdrivetrain']
    drop_features = ['sellercity', 'sellerispriv', 'sellername', 'sellerzip', 'vehbodystyle', 'vehmodel', 'vehpricelabel']

    features = features.drop(drop_features, axis=1)

    # process columns

    features['sellerlistsrc'] = bin_sellerlistsrc(features['sellerlistsrc'])
    features['vehcolorext'] = bin_vehcolorext(features['vehcolorext'])
    features['vehcolorint'] = bin_vehcolorint(features['vehcolorint'])
    features['vehdrivetrain'] = bin_vehdrivetrain(features['vehdrivetrain'])
    features['vehengine'] = bin_vehengine(features['vehengine'])

    vehfeats_df = process_vehfeats(features['vehfeats']) #merge this dataframe later
    features = features.drop('vehfeats', axis=1)

    owners, vehhistory_df = process_vehhistory(features['vehhistory'])
    features = features.drop('vehhistory', axis=1)

    tfidf_data = process_testvehsellernotes(features['vehsellernotes'], tfidf)

    numfeats = features.loc[:, [j for j in features.columns if j in numeric_features]]
    for feat in log_features:

      numfeats[feat + '_log'] = np.log(features.loc[:, feat].add(features.loc[:, feat].pow(2).add(1).pow(0.5)))

    catfeats = features.loc[:, [j for j in features.columns if j in categorical_features]]
    catfeats['sellerrating_round'] = features['sellerrating'].copy().round()
    catfeats['owners'] = owners

    nums_scaled = process_testnumfeats(numfeats, scaler)
    cats_coded = process_testcatfeats(catfeats, encoder)
    tfidf_df = process_tfidf(tfidf_data, tfidf, numfeats.index)

    processed_features = pd.concat([nums_scaled, cats_coded, vehfeats_df, tfidf_df], axis=1)
    return processed_features

def process_testvehsellernotes(series, tfidf):
    series.loc[series.isna()] = 'none'
    tfidf_data = tfidf.transform(series)
    return tfidf_data

def process_testnumfeats(numfeats, scaler):
    nums_scaled = scaler.fit_transform(numfeats)
    nums_scaled = pd.DataFrame(nums_scaled, columns=numfeats.columns, index=numfeats.index)
    return nums_scaled

def process_testcatfeats(catfeats, encoder):
    for feat in catfeats.columns:
      if catfeats.loc[:, feat].dtype != 'O':
        catfeats.loc[:, feat] = catfeats.loc[:, feat].astype('O')

    cats_encoded = encoder.transform(catfeats)
    cats_encoded = pd.DataFrame(cats_encoded.todense(), index=catfeats.index, columns=[item for sublist in encoder.categories_ for item in sublist])
    return cats_encoded

def classification_tuning(hyperparameter_space):
  model = XGBClassifier(n_estimators=int(hyperparameter_space['n_estimators']),
                        learning_rate=hyperparameter_space['learning_rate'],
                        max_depth=int(hyperparameter_space['max_depth']),
                        gamma=hyperparameter_space['gamma'],
                        reg_alpha=int(hyperparameter_space['reg_alpha']),
                        reg_lambda=hyperparameter_space['reg_lambda'],
                        min_child_weight=hyperparameter_space['min_child_weight'],
                        colsample_bytree=hyperparameter_space['colsample_bytree'],
                        objective='multi:softprob',
                        use_label_encoder=False)

  evaluation = [(X_train, y_train), (X_test, y_test)]

  model.fit(X_train, y_train,
            eval_set=evaluation,
            eval_metric='aux',
            early_stopping_rounds=20,
            verbose=False)

  pred = model.predict(X_test)
  pred_proba = model.predict_proba(X_test)

  acc = accuracy_score(y_test, pred)
  auc = roc_auc_score(y_test, pred_proba, multi_class='ovr')
  print('Accuracy: {0:2%}'.format(acc))
  print('AUC: {0:2f}'.format(auc))

  return {'loss':-auc, 'status':STATUS_OK, 'model':model}

def regression_tuning(hyperparameter_space):
  model = XGBClassifier(n_estimators=int(hyperparameter_space['n_estimators']),
                        learning_rate=hyperparameter_space['learning_rate'],
                        max_depth=int(hyperparameter_space['max_depth']),
                        gamma=hyperparameter_space['gamma'],
                        reg_alpha=int(hyperparameter_space['reg_alpha']),
                        reg_lambda=hyperparameter_space['reg_lambda'],
                        min_child_weight=hyperparameter_space['min_child_weight'],
                        colsample_bytree=hyperparameter_space['colsample_bytree'])

  evaluation = [(X_train, y_train), (X_test, y_test)]

  model.fit(X_train, y_train,
            eval_set=evaluation,
            eval_metric='rmse',
            early_stopping_rounds=20,
            verbose=False)

  pred = model.predict(X_test)
  mse = mean_squared_error(y_test, pred)
  r2 = r2_score(y_test, pred)

  print('RMSE: {0:.4f}'.format(mse**0.5))
  print('R2: {0:.4f}'.format(r2))

  return {'loss': -r2, 'status':STATUS_OK, 'model':model}


def optimize(hyperparameter_space,
             features, target,
             regression=False):
  global X_train
  global X_test
  global y_train
  global y_test

  X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)

  if regression == True:
    func = regression_tuning
  else:
    func = classification_tuning

  trials = Trials()
  best = fmin(fn=func,
              space=hyperparameter_space,
              algo=tpe.suggest,
              max_evals=100,
              trials=trials)
  return best

def plot_multiclass_roc(n_classes, target_binarized, pred_proba, ax):
  false_positive_rate = dict()
  true_positive_rate = dict()
  roc_auc = dict()

  for i in range(n_classes):
    false_positive_rate[i], true_positive_rate[i], _ = roc_curve(target_binarized[:,i], pred_proba[:,i])
    roc_auc[i] = auc(false_positive_rate[i], true_positive_rate[i])

    ax.plot(false_positive_rate[i], true_positive_rate[i], label=f'ROC CUrve of Class {i} (AUC={roc_auc[i]:0.2f})')

  ax.plot([0,1], [0,1], 'k--')

  ax.set_xlim([-0.05, 1.0])
  ax.set_ylim([0.0, 1.05])
  ax.set_xlabel('False Positive Rate')
  ax.set_ylabel('True Positive Rate')
  ax.legend()

training_data = training_data.set_index('ListingID')

"""Explore, Clean, and Split Data Into Dependent and Independent Variables"""

# Dependent Variables
training_data.columns = training_data.columns.str.lower()
y = training_data[['vehicle_trim', 'dealer_listing_price']]
y.info()

y.isna().sum()

x = training_data.drop(y.columns, axis=1)
x.info()

x.isna().sum() # finding missing values

y.head()

y['vehicle_trim'].value_counts()

# Here is where I decided to find which makes have common words in their trims,
# such as limited, luxury, AWD,and FWD, and group them by make

training_data.groupby('vehmake')['vehicle_trim'].value_counts()

# Because we have two separate makes and a significant amount of models,
# I decided to split them up for accuracy

cadillac_data = training_data[training_data['vehmake'] == 'Cadillac']
jeep_data = training_data[training_data['vehmake'] == 'Jeep']

# Due to the overlapping models and to shore up the data, I decided here to bin the trims
# into more manageable and accurate variables. I did this by taking the top four trim outputs from each make
# and dropping things such as AWD, FWD, which do not give us enough information to determine the trim

cadillac_data.loc[cadillac_data['vehicle_trim'].str.lower().str.contains('premium') == True, 'vehicle_trim'] = 'Premium'
cadillac_data.loc[cadillac_data['vehicle_trim'].str.lower().str.contains('luxury') == True, 'vehicle_trim'] = 'Luxery'
cadillac_data.loc[cadillac_data['vehicle_trim'].str.lower().str.contains('base') == True, 'vehicle_trim'] = 'Base'
cadillac_data.loc[cadillac_data['vehicle_trim'].str.lower().str.contains('platinum') == True, 'vehicle_trim'] = 'Platinum'

# Here is the output from the code above, which shows that we end up losing a relative small amount of not helpful entries

cadillac_data['vehicle_trim'].value_counts()

# For Jeep, we see that there are more models that do not give enough info as to the trim,
# so we will keep the ones that are identifiable and individual enough from the other entries to be relevent

jeep_trims = ['limited', 'laredo', 'overland', 'altitude', 'summit', 'trailhawk', 'high altitude', 'srt']
jeep_data = jeep_data.loc[jeep_data['vehicle_trim'].str.lower().isin(jeep_trims), :]
jeep_data['vehicle_trim'].value_counts()
jeep_data.info()

# graph to show how new counts of each trim
fig, axes = plt.subplots(ncols=2, figsize=(16,8))
jeep_data['vehicle_trim'].value_counts().sort_values().plot.barh(ax=axes[0])
axes[0].set_title('Jeep Trims')
cadillac_data['vehicle_trim'].value_counts().sort_values().plot.barh(ax=axes[1])
axes[1].set_title('Cadillac Trims')
fig.suptitle('Total Count')

y['dealer_listing_price'].isna().sum()

fig, ax = plt.subplots(figsize=(15,8))
sns.histplot(cadillac_data['dealer_listing_price'], kde=True, color='blue', ax=ax, label='Cadillac')
sns.histplot(jeep_data['dealer_listing_price'], kde=True, color='green', ax=ax, label='Jeep')
ax.legend()
ax.set_title('Listing Price Distriution');

fig, ax = plt.subplots(figsize=(15,8))
sns.histplot(np.log(cadillac_data['dealer_listing_price']), kde=True, color='blue', ax=ax, label='Cadillac')
sns.histplot(np.log(jeep_data['dealer_listing_price']), kde=True, color='green', ax=ax, label='Jeep')
ax.legend()
ax.set_title('Listing Price Distriution');

fig, axes = plt.subplots(ncols=2, figsize=(20,8))
sm.qqplot(cadillac_data['dealer_listing_price'],
          loc=cadillac_data['dealer_listing_price'].mean(),
          scale=cadillac_data['dealer_listing_price'].std(),
          ax=axes[0], line='45')
axes[0].set_title('QQ-Plot of Dealer Listing Price')

temp = np.log(cadillac_data['dealer_listing_price'])
sm.qqplot(temp,
          loc=temp.mean(),
          scale=temp.std(),
          ax=axes[1], line='45')
axes[1].set_title('QQ-Plot of Log-Price')
fig.suptitle('Cadillac');

fig, axes = plt.subplots(ncols=2, figsize=(20,8))
sm.qqplot(jeep_data['dealer_listing_price'],
          loc=jeep_data['dealer_listing_price'].mean(),
          scale=jeep_data['dealer_listing_price'].std(),
          ax=axes[0], line='45')
axes[0].set_title('QQ-Plot of Dealer Listing Price')

temp = np.log(jeep_data['dealer_listing_price'])
sm.qqplot(temp,
          loc=temp.mean(),
          scale=temp.std(),
          ax=axes[1], line='45')
axes[1].set_title('QQ-Plot of Log-Price')
fig.suptitle('Jeep');

def quick_scale(series):
  return series.sub(np.mean(series)).div(np.std(series))

fig, ax = plt.subplots(figsize=(15,8))
sns.histplot(quick_scale(cadillac_data['dealer_listing_price']), kde=True, color='blue', ax=ax, label='Cadillac')
sns.histplot(quick_scale(jeep_data['dealer_listing_price']), kde=True, color='green', ax=ax, label='Jeep')
ax.legend()
ax.set_title('Listing Price Distriution of Scaled  Log-Price');

fig, axes = plt.subplots(ncols=2, figsize=(20,8))
temp = quick_scale(np.log(cadillac_data['dealer_listing_price']))
sm.qqplot(temp, line='45', ax=axes[0])
axes[0].set_title('QQ-Plot of Scaled Log Price (Cadillac)')

temp = quick_scale(np.log(jeep_data['dealer_listing_price']))
sm.qqplot(temp, line='45', ax=axes[1])
axes[0].set_title('QQ-Plot of Scaled Log Price (Jeep)');

cadillac_data.loc[:, 'log_price'] = np.log(cadillac_data.loc[:, 'dealer_listing_price'])
jeep_data.loc[:, 'log_price'] = np.log(jeep_data.loc[:, 'dealer_listing_price'])

# Independent Variables
# Look at all features and determine how to process

whole_data = pd.concat([jeep_data, cadillac_data])
whole_data.dropna(inplace=True)
whole_data.info()

test_index = np.random.choice(whole_data.index,
                             size=whole_data.index.size // 5,
                             replace=False)
train_index = whole_data.index[[i not in test_index for i in whole_data.index]]
whole_data, test_data = whole_data.loc[train_index,:].copy(), whole_data.loc[test_index,:].copy()

# Review Variables and make them either categorical, numeric, or remove them
categorical_features = []
numeric_features = []
drop_features = []
data_cols = whole_data.columns[:26]
data_cols

# Review each feature to determine if needed for analysis

i = 0
temp = whole_data[data_cols[i]].copy()
temp.head()

(temp.value_counts() / temp.size).head(10) * 100

# drop city column, not relevant due to number of cities
drop_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head(), temp.value_counts()

# drop this variable as well
drop_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts() / temp.size *100

temp = bin_sellerlistsrc(temp)
temp.value_counts() / temp.size

# Save this in a list of categorical features
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts() / temp.size *100

# drop this variable as well
drop_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

fig, ax = plt.subplots(figsize=(15,8))
sns.histplot(temp, kde=True, ax=ax);

# bin this one by rounding, but also keep the original as numeric
categorical_features.append(data_cols[i] + '_round')
numeric_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

fig, axes = plt.subplots(ncols=2, figsize=(20,8))
sns.histplot(temp, kde=True, ax=axes[0])
axes[0].set_title('Original Distribution')

sns.histplot(np.log(temp), kde=True, ax=axes[1])
axes[1].set_title('Log Distribution');

# Do a log transformation and keep this variable numeric
numeric_features.append(data_cols[i] + '_log')
i+= 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts().head(5).append(temp.value_counts().tail(5))

# Keep these and one-hot encode them and select variables later
categorical_features.append(data_cols[i])
i+= 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.unique().size

# Drop this feature, brings too much complication to be useful
# If this could be used to make a heat map of sorts at a later time, that could prove quite useful
drop_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

# Because every variable here is SUV, this is dropped as well
drop_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

# Keep as categorical
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp = bin_vehcolorext(temp)
temp.value_counts()

# Make categorical feature
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

temp = bin_vehcolorint(temp)
temp.value_counts()

# bin and keep as categorocal variable
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp = bin_vehdrivetrain(temp)
temp.value_counts()

# bin and keep as categorocal variable
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp = bin_vehengine(temp)
temp.value_counts()

# keep as categorical
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

# one hot and keep dataframe (fix mispelling of aluminum as well)
# assign 1 or 0 to features
temp2 = process_vehfeats(temp)
temp2.head()

temp2.mean()

i += 1
temp = whole_data[data_cols[i]].copy()
temp.head()

# keep as categorical
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

# this contains owner history, buyback, accidents, and type of use
# will make new dataframe based on this for predicting purposes

temp2_cat, temp2_oh = process_vehhistory(temp)
temp2_cat.mean()

temp2_oh.mean()

i += 1
temp = whole_data[data_cols[i]].copy()
temp.head()

fig, axes = plt.subplots(ncols=2, figsize=(20,8))
sns.histplot(temp, kde=True, ax=axes[0])
axes[0].set_title('Original Distribution')
sns.histplot(np.log(temp), kde=True, ax=axes[1])
axes[1].set_title('Log Distribution');

# Keep log distribution as numeric
numeric_features.append(data_cols[i])
numeric_features.append(data_cols[i] + '_log')

i += 1
temp = whole_data[data_cols[i]].copy()
temp.head()

# Keep as categorical for now
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

fig, axes = plt.subplots(ncols=2, figsize=(20,8))
sns.histplot(temp, kde=True, ax=axes[0])
axes[0].set_title('Distribution')
sm.qqplot(temp,
          loc=temp.mean(),
          scale=temp.std(),
          line='45',
          ax=axes[1])
axes[1].set_title('QQ-Plot');

# leave as numeric feature
numeric_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

# drop due to one column already being jeep and the other being cadillac
drop_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

# Keep as categorical
categorical_features.append(data_cols[i])
i += 1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.iloc[1]

# Lots of information about trim and price, however it is difficult to parse with time allowed
tfidf_data = process_vehsellernotes(temp)
tfidf_data

i += 1
temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

# this is all similar so it will be dropped
drop_features.append(data_cols[i])
i +=1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

# drop this as well as they are all quite similar
drop_features.append(data_cols[i])
i +=1

temp = whole_data[data_cols[i]].copy()
temp.head()

temp.value_counts()

# Keep as categorical
categorical_features.append(data_cols[i])
i += 1

print(numeric_features)

print(categorical_features)

print(drop_features)

whole_data.head()

features = whole_data.iloc[:, :-3]
target1 = whole_data['vehicle_trim']
target2 = whole_data['dealer_listing_price']
target3 = whole_data['log_price']

scaler, encoder, tfidf, processed_features = preprocess_features(features)

processed_features.info()

mutual_info = mutual_info_regression(processed_features, target2)
mutual_info = pd.Series(mutual_info, index=processed_features.columns)

"""Use the Training Set to Model the Trim"""

trim_encoder = LabelEncoder()
target1_coded = trim_encoder.fit_transform(target1.values)

cv = KFold(shuffle=True, random_state=1).split(processed_features.values, target1_coded)

roc_scores = dict()
preds = dict()
preds_proba = dict()
test_Xs = dict()
test_ys = dict()

for i, (train_index, test_index) in enumerate(cv):
  X_train = processed_features.values[train_index]
  y_train = target1_coded[train_index]

  test_Xs[i] = processed_features.values[test_index]
  test_ys[i] = target1_coded[test_index]

  model = XGBClassifier(use_label_encoder=False, eval_metrics='merror')
  fit_model = model.fit(X_train, y_train)

  preds[i] = fit_model.predict(test_Xs[i])
  preds_proba[i] = fit_model.predict_proba(test_Xs[i])

  roc_scores[i] = roc_auc_score(test_ys[i], preds_proba[i], multi_class='ovr')

[print(f'Fold {i}, AUC Score = {score}') for i, score in enumerate(roc_scores.values())];

"""The AUC Scores look greatshow we have trained the models, now time to make and export our predictions on the test data:"""

# Import the test data and review for missing information, clean up accordingly using same process as above
test_data = pd.read_csv('/content/Test_Dataset.csv')
final_answers = test_data.iloc[:, -3:]
test_data = test_data.drop(final_answers.columns, axis=1)
test_data.columns = test_data.columns.str.lower()

test_data.info() #'sellercity', 'sellerispriv', 'sellername', 'sellerzip', 'vehbodystyle', 'vehmodel', 'vehtype', 'vehtransmission'

test_data.isna().sum()

test_features = process_testfeatures(scaler, encoder, tfidf, test_data)

final_answers.info()

print(test_data.columns)

drop_features = ['sellercity', 'sellerispriv', 'sellername', 'sellerzip', 'vehbodystyle', 'vehmodel', 'vehtype', 'vehtransmission']
for feature in drop_features:
    if feature not in test_data.columns:
        print(f"Feature '{feature}' not found in DataFrame")

pred_labels_test = fit_model.predict(test_features.values)
pred_proba_test = pd.DataFrame(fit_model.predict_proba(test_features.values), index=test_features.index)
pred_trims_test = pd.Series(trim_encoder.inverse_transform(pred_labels_test), index=test_features.index)

# Model listing price
onehot_trim = pd.get_dummies(target1)
price_features = pd.concat([processed_features, onehot_trim], axis=1)
price_features.info()

cv = KFold(shuffle=True, random_state=20).split(price_features.values, target2.values)

r2_scores = dict()
scores_dict = dict()
preds = dict()
test_Xs = dict()
test_ys = dict()

for i, (train_index, test_index) in enumerate(cv):
  X_train = price_features.values[train_index]
  y_train = target2.values[train_index]

  test_Xs[i] = price_features.values[test_index]
  test_ys[i] = target2.values[test_index]

  model = XGBRegressor()
  fit_model = model.fit(X_train, y_train)

  preds[i] = fit_model.predict(test_Xs[i])

  r2_scores[i] = r2_score(test_ys[i], preds[i])
  scores_dict[i] = {'rmse':np.sqrt(mean_squared_error(test_ys[i], preds[i])),
                    'mae': mean_absolute_error(test_ys[i], preds[i]),
                    'medae': median_absolute_error(test_ys[i], preds[i])}

[print(f'Fold {i}, R2 Score = {score}') for i, score in enumerate(r2_scores.values())];

# create a matrix of expected prices
(test_features.columns == price_features.columns[:test_features.columns.size]).mean()

# avoid repeating column names
test_price_features = test_features.copy()
for i in range(195, 208):
  colname = price_features.columns[i]
  newcol = pd.Series(0, index=test_features.index, name=colname)
  test_price_features = pd.concat([test_price_features, newcol], axis=1)

exp_prices = dict()
for i in range (195, 208):
  temp_feats = test_price_features.copy()
  temp_feats.iloc[:, i] = 1
  exp_prices[i] = fit_model.predict(temp_feats)

exp_prices = pd.DataFrame(exp_prices, index=test_features.index)

print(exp_prices.info)

pred_prices_test = pd.Series(index=test_features.index, dtype =float)
for idx in test_features.index:
  pred_prices_test.loc[idx] = (exp_prices.loc[idx, :].values * pred_proba_test.loc[idx, :].values).sum()

final_pred = pd.concat([pred_trims_test, pred_prices_test], axis=1)
final_pred.columns = ['vehicle_trim', 'dealer_listing_price']
final_pred.to_csv('Hull_Prediction_Results.csv')

print(classification_report(final_pred['vehicle_trim'],
                            pred_trims_test))

test_roc = roc_auc_score(final_pred['vehicle_trim'],
                     pred_proba_test,
                     multi_class='ovr')
print(f'Area Under the Curve of Test Set : {test_roc:.4%}')

mean_r2_scores = mean(r2_scores.values())
print(f'R2 Score : {mean_r2_scores:.4%}')